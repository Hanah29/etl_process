{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4aecc444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\herme\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.3.3)\n",
      "Requirement already satisfied: psycopg2-binary in c:\\users\\herme\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.9.11)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\herme\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\herme\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\herme\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\herme\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\herme\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\herme\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\herme\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas psycopg2-binary python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "952ca701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##  Step 1. Setup and Import Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from psycopg2 import extras\n",
    "from dotenv import load_dotenv\n",
    "from io import StringIO\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b99f1af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##  Step 2. Load Environment Variables\n",
    "\n",
    "# Load variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Database settings\n",
    "DB_NAME = os.getenv(\"PG_DBNAME\")\n",
    "DB_USER = os.getenv(\"PG_USER\")\n",
    "DB_PASSWORD = os.getenv(\"PG_PASSWORD\")\n",
    "DB_HOST = os.getenv(\"PG_HOST\")\n",
    "DB_PORT = os.getenv(\"PG_PORT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88ad2904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Environment variables loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# File settings\n",
    "CSV_PATH = os.getenv(\"CSV_PATH\")\n",
    "CHUNK_SIZE = int(os.getenv(\"CHUNK_SIZE\", \"200000\"))\n",
    "TABLE_NAME = os.getenv(\"TABLE_NAME\", \"people\")\n",
    "\n",
    "print(\" Environment variables loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d15393e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##  Step 3. Define Database Table Schema\n",
    "\n",
    "create_table_query = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {TABLE_NAME} (\n",
    "    row_index INTEGER,\n",
    "    user_id VARCHAR(50),\n",
    "    first_name TEXT,\n",
    "    last_name TEXT,\n",
    "    sex VARCHAR(10),\n",
    "    email TEXT,\n",
    "    phone TEXT,\n",
    "    date_of_birth DATE,\n",
    "    job_title TEXT\n",
    ");\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20c94cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##  Step 4. Define ETL Functions\n",
    "\n",
    "def extract_csv(csv_path, chunk_size):\n",
    "    \"\"\"Extracts data from a large CSV file in chunks.\"\"\"\n",
    "    print(f\"Extracting data from: {csv_path}\")\n",
    "    return pd.read_csv(csv_path, chunksize=chunk_size, dtype=str, na_values=[\"\", \"NA\", \"N/A\"])\n",
    "\n",
    "\n",
    "def transform_chunk(chunk):\n",
    "    \"\"\"Cleans and transforms a single chunk of data.\"\"\"\n",
    "    column_map = {\n",
    "        \"Index\": \"row_index\",\n",
    "        \"User Id\": \"user_id\",\n",
    "        \"First Name\": \"first_name\",\n",
    "        \"Last Name\": \"last_name\",\n",
    "        \"Sex\": \"sex\",\n",
    "        \"Email\": \"email\",\n",
    "        \"Phone\": \"phone\",\n",
    "        \"Date of birth\": \"date_of_birth\",\n",
    "        \"Job Title\": \"job_title\"\n",
    "    }\n",
    "\n",
    "    # Rename columns\n",
    "    chunk.rename(columns=column_map, inplace=True)\n",
    "\n",
    "    # Convert date format\n",
    "    if \"date_of_birth\" in chunk.columns:\n",
    "        chunk[\"date_of_birth\"] = pd.to_datetime(chunk[\"date_of_birth\"], errors=\"coerce\")\n",
    "\n",
    "    # Fill missing values\n",
    "    chunk.fillna({\"first_name\": \"Unknown\", \"last_name\": \"Unknown\", \"job_title\": \"Unspecified\"}, inplace=True)\n",
    "\n",
    "    # Reorder columns\n",
    "    expected_columns = [\n",
    "        \"row_index\", \"user_id\", \"first_name\", \"last_name\",\n",
    "        \"sex\", \"email\", \"phone\", \"date_of_birth\", \"job_title\"\n",
    "    ]\n",
    "    chunk = chunk[[col for col in expected_columns if col in chunk.columns]]\n",
    "\n",
    "    print(f\" Transformed chunk with {len(chunk)} rows\")\n",
    "    return chunk\n",
    "\n",
    "\n",
    "def copy_from_stringio(conn, df, table):\n",
    "    \"\"\"Efficiently loads a Pandas DataFrame into PostgreSQL using COPY.\"\"\"\n",
    "    buffer = StringIO()\n",
    "    df.to_csv(buffer, index=False, header=False)\n",
    "    buffer.seek(0)\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        cursor.copy_from(buffer, table, sep=\",\", null=\"\")\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(\" Error during COPY:\", e)\n",
    "    finally:\n",
    "        cursor.close()\n",
    "\n",
    "\n",
    "def load_to_postgres(conn, chunk, table_name):\n",
    "    \"\"\"Loads one chunk of data into PostgreSQL.\"\"\"\n",
    "    copy_from_stringio(conn, chunk, table_name)\n",
    "    print(f\" Loaded {len(chunk)} records into '{table_name}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e9746d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Connected to PostgreSQL!\n",
      "Table verified or created successfully.\n",
      "Extracting data from: C:/Users/herme/Downloads/people-2000000/people-2000000.csv\n",
      "\n",
      " Processing Chunk 1 with 200000 rows\n",
      " Transformed chunk with 200000 rows\n",
      " Chunk 1 inserted successfully.\n",
      "\n",
      " Processing Chunk 2 with 200000 rows\n",
      " Transformed chunk with 200000 rows\n",
      " Chunk 2 inserted successfully.\n",
      "\n",
      " Processing Chunk 3 with 200000 rows\n",
      " Transformed chunk with 200000 rows\n",
      " Chunk 3 inserted successfully.\n",
      "\n",
      " Processing Chunk 4 with 200000 rows\n",
      " Transformed chunk with 200000 rows\n",
      " Chunk 4 inserted successfully.\n",
      "\n",
      " Processing Chunk 5 with 200000 rows\n",
      " Transformed chunk with 200000 rows\n",
      " Chunk 5 inserted successfully.\n",
      "\n",
      " Processing Chunk 6 with 200000 rows\n",
      " Transformed chunk with 200000 rows\n",
      " Chunk 6 inserted successfully.\n",
      "\n",
      " Processing Chunk 7 with 200000 rows\n",
      " Transformed chunk with 200000 rows\n",
      " Chunk 7 inserted successfully.\n",
      "\n",
      " Processing Chunk 8 with 200000 rows\n",
      " Transformed chunk with 200000 rows\n",
      " Chunk 8 inserted successfully.\n",
      "\n",
      " Processing Chunk 9 with 200000 rows\n",
      " Transformed chunk with 200000 rows\n",
      " Chunk 9 inserted successfully.\n",
      "\n",
      " Processing Chunk 10 with 200000 rows\n",
      " Transformed chunk with 200000 rows\n",
      " Chunk 10 inserted successfully.\n",
      "\n",
      " ETL Pipeline completed successfully!\n",
      " Database connection closed.\n"
     ]
    }
   ],
   "source": [
    "# ## Step 5. Execute the ETL Pipeline (Safe Bulk Insert)\n",
    "\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from psycopg2 import extras\n",
    "\n",
    "try:\n",
    "    # Connect to PostgreSQL\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=DB_NAME,\n",
    "        user=DB_USER,\n",
    "        password=DB_PASSWORD,\n",
    "        host=DB_HOST,\n",
    "        port=DB_PORT\n",
    "    )\n",
    "    print(\" Connected to PostgreSQL!\")\n",
    "\n",
    "    # Create table if not exists\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(create_table_query)\n",
    "        conn.commit()\n",
    "        print(\"Table verified or created successfully.\")\n",
    "\n",
    "    # Process CSV in chunks\n",
    "    for i, chunk in enumerate(extract_csv(CSV_PATH, CHUNK_SIZE), start=1):\n",
    "        print(f\"\\n Processing Chunk {i} with {len(chunk)} rows\")\n",
    "\n",
    "        # Transform the chunk\n",
    "        transformed = transform_chunk(chunk)\n",
    "\n",
    "        # Convert DataFrame to list of tuples\n",
    "        data_tuples = list(transformed.itertuples(index=False, name=None))\n",
    "\n",
    "        # Prepare insert query\n",
    "        columns_str = ', '.join(f'\"{col}\"' for col in transformed.columns)\n",
    "        insert_query = f\"INSERT INTO {TABLE_NAME} ({columns_str}) VALUES %s\"\n",
    "\n",
    "        # Bulk insert using execute_values (safe for commas inside text)\n",
    "        try:\n",
    "            with conn.cursor() as cursor:\n",
    "                extras.execute_values(cursor, insert_query, data_tuples, page_size=10_000)\n",
    "                conn.commit()\n",
    "            print(f\" Chunk {i} inserted successfully.\")\n",
    "        except Exception as e:\n",
    "            conn.rollback()\n",
    "            print(f\" Error inserting chunk {i}: {e}\")\n",
    "            break\n",
    "\n",
    "    print(\"\\n ETL Pipeline completed successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\" Error:\", e)\n",
    "\n",
    "finally:\n",
    "    if 'conn' in locals() and conn:\n",
    "        conn.close()\n",
    "        print(\" Database connection closed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
